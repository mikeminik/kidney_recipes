import numpy as np
import pandas as pd
import requests
from bs4 import BeautifulSoup
import os

from selenium import webdriver
from selenium.webdriver.common.by import By
import time





def get_file_name(url):
    # create a "file name" that is derived from the URL that is primarily used to identify the ingredients
    file_name = url.split('recipe/')[1].strip('/')
    return file_name


def extract_urls_from_soup(soup):
    # create a list of individual recipe URLs from "visible" HTML/soup
    urls = []
    cards = soup.find_all('div', {'class' : 'right'})
    for i in range(1,len(cards)):
        urls.append(soup.find_all('div', {'class' : 'right'})[i].attrs['onclick'].split('=')[1].replace("'","").strip())
    return urls


def extract_recipe_info(url, omit_tags = []):
    # given a recipe page's url, extract key recipe info
    
    time.sleep(3)
    print(f"Getting information from: {url}")
    # extracts ingredients from a specific recipes
    
    ingredients_text = []
    tags = []
    
    # make request and soup
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
    res = requests.get(url, headers=headers)
    if res.status_code != 200:
        print(f"{url} request status: {res.status_code}")
    soup = BeautifulSoup(res.content)
    
    # get the tags
    
    for li in soup.find_all('ul', {'class':'recipe-nutrients'})[0].find_all('li'):
        tags.append(li['class'][0])
    # if tags are not ideal, skip the rest
    if not any(string in tags for string in omit_tags):
        # get the serving size
        serving_size = soup.find_all('div', {'class', 'elementor-element elementor-element-79fbba0c elementor-widget elementor-widget-text-editor'})[0].find('div').text.strip()


        # Ingredients: focus in on the element that contains ingredients using 'li' first
        ingredients_div = soup.find_all('div', {'class': 'elementor-element elementor-element-6eb817d elementor-widget elementor-widget-text-editor'})
        ingredient_items = [ result.find_all('p') for result in ingredients_div]

        # Ingredients: if first pass above yields no results, try for 'li' elements instead
        if len(ingredient_items[0]) == 0:
            ingredient_items = [ result.find_all('li') for result in ingredients_div]

        [ingredients_text.append(l.text) for l in ingredient_items[0]]
        
        # nutritional info:
        nutrition_titles = soup.find_all('p', {'class', 'nutritional-item-title'})
        nutrition_data = soup.find_all('p', {'class', 'nutritional-item-data'})
        calories = None
        satfat = None
        protein = None
        sodium = None
        potassium = None
        phosphorus = None
        
        for i,j in zip(nutrition_titles, nutrition_data):
            if i.text.lower().strip() == 'calories':
                calories = j.text
            elif i.text.lower().strip() == 'saturated fat':
                satfat = j.text
            elif i.text.lower().strip() == 'protein':
                protein = j.text
            elif i.text.lower().strip() == 'sodium':
                sodium = j.text
            elif i.text.lower().strip() == 'potassium':
                potassium = j.text
            elif i.text.lower().strip() == 'phosphorus':
                phosphorus = j.text
        nutrition = {'calories': calories,
                     'saturated fat': satfat,
                     'protein': protein,
                     'sodium': sodium,
                     'potassium': potassium,
                     'phosphorus': phosphorus
                    }
        
        return serving_size, ingredients_text, nutrition
    print(f"Alert: Skipped recipe at url: {url} due to non-ideal nutrient tags")
    return None, None, None


def create_files_from_urls(urls, output_relative_directory):
    
    # run the ingredient extractor secondary func
    for u in urls:
        file_name = get_file_name(u)
        item_ingredients = extract_ingredients(u)
        
        if len(item_ingredients) > 0:
            print('Printing: ' + file_name)
        else:
            print(f"WARNING: {file_name} ingredients list is empty")
        
        # create text file to directory

            
        with open(os.path.join(output_relative_directory, file_name), 'w', encoding='utf-8') as f:
            # for i in item_description:
            #     line = f"{i}\n"
            #     f.write(line)
            for i in item_ingredients:
                line = f"{i}\n"
                f.write(line)





# browser = webdriver.Chrome()
# url = "https://kitchen.kidneyfund.org/find-recipes/"
# browser.get(url)
# time.sleep(2)
# click = 0
# browser.execute_script("window.scrollTo(0,document.body.scrollHeight)")
# time.sleep(4)
# while browser.find_element(By.CSS_SELECTOR, "button.wpgb-button.wpgb-load-more"):
#     next_button = browser.find_element(By.CSS_SELECTOR, "button.wpgb-button.wpgb-load-more")
#     next_button.click()
#     click+=1
#     print(f"Completed click: {click}")
#     time.sleep(3)
#     browser.execute_script("window.scrollTo(0,document.body.scrollHeight)")
#     time.sleep(2)
#     html = browser.page_source
#     with open('kidney_kitchen_html.txt','w', encoding='utf-8') as f:
#         f.write(html)







with open ('../data/kidney-kitchen/kidney_kitchen_html.txt', 'r', encoding='utf-8') as f:
    html = f.read()
soup = BeautifulSoup(html)





recipes = extract_urls_from_soup(soup)





all_recipes = []


# uncomment below if DataFramescraping is necessary
# warning: may need to take note of any failures and resume where the error occurred
# bad_tags = ['high-protein', 'high-sodium', 'high-potassium']
# for url in recipes:
#     serving, ingredients, nutrition = extract_recipe_info(url, bad_tags)
#     if not any(value == None for value in [serving, ingredients,nutrition]):
#         all_recipes.append({'url': url,
#                             'serving_size': serving,
#                             'ingredients_raw': ingredients,
#                             'calories': nutrition['calories'],
#                             'saturated fat': nutrition['saturated fat'],
#                             'protein': nutrition['protein'],
#                             'sodium': nutrition['sodium'],
#                             'potassium': nutrition['potassium'],
#                             'phosphorus': nutrition['phosphorus'] 
#                            })


# all_recipes_df = pd.DataFrame(all_recipes)
# all_recipes_df.to_csv('kidney-kitchen-recipes-with-ingredients-and-nutrition.csv')


headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
url = "https://kitchen.kidneyfund.org/recipe/sweet-chiffon-pie/"
res = requests.get(url, headers=headers)
soup = BeautifulSoup(res.content)


tags = []
for li in soup.find_all('ul', {'class':'recipe-nutrients'})[0].find_all('li'):
    tags.append(li['class'][0])


tags


test_tags = ['high-protein', 'high-sodium']


if not any(string in tags for string in test_tags ):
    print('Good to go')


soup.find_all('div', {'class', 'elementor-element elementor-element-79fbba0c elementor-widget elementor-widget-text-editor'})[0].find('div').text.strip()


for tag in tags:
    print(tag['class'][0])


for r in recipes:
    if 'tarragon' in r:
        print(recipes.index(r))


url = 'https://kitchen.kidneyfund.org/recipe/sweet-chiffon-pie/'
test_tags = ['high-protein', 'high-sodium']
serving, ingredients, nutrition = extract_ingredients(url, test_tags)


all_recipes = []





pd.DataFrame(all_recipes)


serving


ingredients


nutrition_titles = soup.find_all('p', {'class', 'nutritional-item-title'})
nutrition_data = soup.find_all('p', {'class', 'nutritional-item-data'})
for i,j in zip(nutrition_titles, nutrition_data):
    print(i.text)
    print(j.text)


nutrition_data = soup.find_all('p', {'class', 'nutritional-item-data'})
for i in nutrition_data:
    print(i.find('span').text)



for url in recipes:
    


#soup.find_all('div', {'class' : 'right'})[1].attrs['onclick']


recipes = []
for i in range(1,len(cards)):
    recipes.append(soup.find_all('div', {'class' : 'right'})[i].attrs['onclick'].split('=')[1].replace("'","").strip())
    





sub_url = recipes[0]
sub_res = requests.get(sub_url, headers=headers)
sub_res.status_code



sub_soup = BeautifulSoup(sub_res.content)


divs = sub_soup.find_all('div', {'class': 'elementor-element elementor-element-6eb817d elementor-widget elementor-widget-text-editor'})


len(divs)


divs[0].find_all('p')


extract_description_and_ingredients(recipes[0])



