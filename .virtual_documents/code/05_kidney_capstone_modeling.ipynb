import os
import shutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import keras

from keras import layers
from keras import regularizers
from keras import metrics
from keras.saving import save_model
from keras.callbacks import ModelCheckpoint, EarlyStopping



if not os.path.exists('../data/generated/text_files/val/'):
    os.mkdir('../data/generated/text_files/val')
    
val_dir = f"../data/generated/text_files/val/"
train_dir = f"../data/generated/text_files/train/"






# Taking special care to make sure that augmented recipes (variants of existing ones) don't end up in both
# train AND validation, we selectively create val from the end of the augmented data so there is no
# leakage




if not os.path.exists(os.path.join(val_dir, 'pos')):
    os.mkdir(os.path.join(val_dir, 'pos'))

kf_files = os.listdir(os.path.join(train_dir, 'pos'))
kf_test_files = kf_files[:1000]

for f in kf_test_files:
  shutil.move(f"{train_dir}/pos/{f}", f"{val_dir}/pos/{f}")


# Non kidney friendly recipes were not augmented and therefore the validation set can be random

if not os.path.exists(os.path.join(val_dir, 'neg')):
  os.mkdir(os.path.join(val_dir, 'neg'))

nkf_files = os.listdir(os.path.join(train_dir, 'neg'))
n_train = len(nkf_files)
nkf_test_files = np.random.choice(nkf_files, 1000, replace = False)

for f in nkf_test_files:
  shutil.move(f"{train_dir}/neg/{f}", f"{val_dir}/neg/{f}")


len(os.listdir(os.path.join(val_dir,'pos')))



len(os.listdir(os.path.join(val_dir,'neg')))








# Create Keras datasets from recipe files

batch_size = 32
train_ds = keras.utils.text_dataset_from_directory(train_dir, batch_size=batch_size)
val_ds = keras.utils.text_dataset_from_directory(val_dir, batch_size=batch_size)



# extract just the vocabulary

text_only_train_ds = train_ds.map(lambda x,y : x)





# use manual vectorization 

max_length = 100
max_tokens = 20_000

text_vectorization = keras.layers.TextVectorization(
    max_tokens = max_tokens,
    output_mode = 'int',
    output_sequence_length=max_length
)

text_vectorization.adapt(text_only_train_ds)


# with open('text_vectorization.pkl', 'wb') as f:
#     pickle.dump({'config': text_vectorization.get_config(),
#                  'weights': text_vectorization.get_weights()}, f)


text_vectorization('''
100 grams beans
1 cup sugar''')


# map the vocabulary to an int, keep the label as is

int_mapper = lambda x,y: (text_vectorization(x), y)


int_train_ds = train_ds.map(int_mapper, num_parallel_calls=4 )
int_val_ds = val_ds.map(int_mapper, num_parallel_calls=4 )


# confirm Tensor shape - will factor into predictions
for x,y in int_train_ds:
    break
x.shape
# 32 for batch size, 100 is max length





# compile the model

inputs = keras.Input(shape=(None,))
embedded = layers.Embedding(
    input_dim=max_tokens,
    output_dim=256,
    mask_zero = True
)(inputs)
x = layers.Bidirectional(layers.LSTM(32, kernel_regularizer = regularizers.L1L2(l1=0.01, l2=0.01)) )(embedded)
x = layers.Dropout(0.5)(x)
x = layers.Dense(32, activation='relu')(x)
outputs = layers.Dense(1, activation='sigmoid')(x)
model_1 = keras.Model(inputs,outputs)
model_1.compile(optimizer='rmsprop', loss='bce', metrics=[metrics.Precision(), 'acc'])
model_1.summary()


# fit the model and record one model per epoch

filepath = "models/manual_vectors/manual_vectors_{epoch}.keras"
mc = ModelCheckpoint(filepath, monitor='val_precision', mode='max', save_best_only=False)
res = model_1.fit(
    int_train_ds,
    validation_data=int_val_ds,
    epochs=10,
    callbacks=[mc]
)





# Visualize stats
val_acc = res.history['va_acc']
val_precision = res.history['val_precision']
plt.figure(figsize=(12, 8))
plt.title('Manually Trained Vectors')

plt.plot(val_acc, label='Validation accuracy', color='navy')
plt.plot(val_precision, label='Validation precision', color='skyblue')
plt.xlabel('Epochs')
plt.legend();





embeddings_ix = {}

# create a dictionary that contains each word in vocabulary and its coefs
with open('../data/GloVe/glove.6B.100d.txt', 'r', encoding='utf-8') as f:
  for line in f:
    word, coefs = line.split(maxsplit=1)
    coefs = np.fromstring(coefs, 'f', sep=' ')
    embeddings_ix[word] = coefs





embedding_dim = 100

# get the vocab from the vectorization from earlier
vocab = text_vectorization.get_vocabulary()

# create a dictionary from enumerating the words in vocab
word_ix = dict(enumerate(vocab))
word_ix = {word: i for i, word in word_ix.items()}


print(word_ix['sodium'])


# create an array of all zeroes of size 20_000 x 100
embedding_mx = np.zeros((max_tokens, embedding_dim))

# for each word, create an embed vector derived from GloVe
for word, i in word_ix.items():
  if i < max_tokens:
    embed_vector = embeddings_ix.get(word)

  if embed_vector is not None:
    embedding_mx[i, :] = embed_vector


# sanity check - word sodium
embedding_mx[158, :5]


embeddings_ix['sodium'][:5]


# create a glove layer to replace custom embedding
glove_layer = layers.Embedding(
    max_tokens,
    embedding_dim,
    embeddings_initializer=keras.initializers.Constant(embedding_mx),
    trainable=False,
    mask_zero=True
)


#compile new model
inputs = keras.Input(shape=(None,))
embedded = glove_layer(inputs)
x = layers.Bidirectional(layers.LSTM(32, kernel_regularizer = regularizers.L1L2(l1=0.01, l2=0.01)))(embedded)
x = layers.Dropout(0.5)(x)
x = layers.Dense(32, activation='relu')(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation='sigmoid')(x)
model2 = keras.Model(inputs,outputs)
model2.compile(optimizer='rmsprop', loss='bce', metrics=[metrics.Precision(),'acc'])
model2.summary()


filepath = "models/glove6B-100d/glove6B-100d_{epoch}.keras"
mc = ModelCheckpoint(filepath, monitor='val_precision', mode='max', save_best_only=False)
# es = EarlyStopping(monitor='acc', mode='max', patience = 3, min_delta = 0.01)
res = model2.fit(
    int_train_ds,
    validation_data=int_val_ds,
    epochs=10,
    callbacks=[mc]
)




# Visualize stats
val_acc = res.history['val_acc']
val_precision = res.history['val_precision_1']
plt.figure(figsize=(12, 8))
plt.title('GloVe 6B 100d Vectors')

plt.plot(val_acc, label='Validation accuracy', color='navy')
plt.plot(val_precision, label='Validation precision', color='skyblue')
plt.xlabel('Epochs')
plt.legend();





embeddings_ix = {}

with open('../data/GloVe/glove.42B.300d.txt', 'r', encoding='utf-8') as f:
  for line in f:
    word, coefs = line.split(maxsplit=1)
    coefs = np.fromstring(coefs, 'f', sep=' ')
    embeddings_ix[word] = coefs





embedding_dim = 300

vocab = text_vectorization.get_vocabulary()
word_ix = dict(enumerate(vocab))
word_ix = {word: i for i, word in word_ix.items()}


print(word_ix['sodium'])


embedding_mx = np.zeros((max_tokens, embedding_dim))

for word, i in word_ix.items():
  if i < max_tokens:
    embed_vector = embeddings_ix.get(word)

  if embed_vector is not None:
    embedding_mx[i, :] = embed_vector


embedding_mx[158, :5]


embeddings_ix['sodium'][:5]


glove_layer = layers.Embedding(
    max_tokens,
    embedding_dim,
    embeddings_initializer=keras.initializers.Constant(embedding_mx),
    trainable=False,
    mask_zero=True
)


inputs = keras.Input(shape=(None,))
embedded = glove_layer(inputs)
x = layers.Bidirectional(layers.LSTM(32, kernel_regularizer = regularizers.L1L2(l1=0.01, l2=0.01)))(embedded)
x = layers.Dropout(0.5)(x)
x = layers.Dense(32, activation='relu')(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation='sigmoid')(x)
model_3 = keras.Model(inputs,outputs)
model_3.compile(optimizer='rmsprop', loss='bce', metrics=[metrics.Precision(), 'acc'])
model_3.summary()


filepath = "models/glove42B-300d/glove42B-300d_{epoch}.keras"
mc = ModelCheckpoint(filepath, save_best_only=False)
res = model_3.fit(
    int_train_ds,
    validation_data=int_val_ds,
    epochs=10,
    callbacks=[mc]
)




# Visualize stats
val_acc = res.history['val_acc']
val_precision = res.history['val_precision_2']
plt.figure(figsize=(12, 8))
plt.title('GloVe 42B 300d Vectors')

plt.plot(val_acc, label='Validation accuracy', color='navy')
plt.plot(val_precision, label='Validation precision', color='skyblue')
plt.xlabel('Epochs')
plt.legend();



